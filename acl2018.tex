%
% File acl2018.tex
%
%% Based on the style files for ACL-2017, with some changes, which were, in turn,
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper,onecolumn]{article}
\usepackage[hyperref]{acl2018}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Automatic Summarization: State-of-the-art review}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@tum.de} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@tum.de} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
This paper provides an overview of the most prominent algorithms for automatic summarization that were proposed in the last years.
\end{abstract}


\section{Introduction}
With large amounts of text available online, the amount of which is growing rapidly everyday, summarizing this content becomes necessary to help users handle such information overload and better perceive information.
Automatic summarization is the process of automatically shortening the content of an information source in a way that retains its most important information.
The goal of summarizers is to produce concise and fluent summary which would allow people to understand the content of the input without reading the entire text.
A good summary must be concise and fluent, capture all the important topics, but not contain repetitions of the same information.

For a long time extractive techniques of text summarization have been the primary focus of research in the field.
However, in the past years there have been less major advances in extractive text summarization.
Most of the research in this area proposes either an enhancement of one of the available extractive approaches, or an ensemble of several previously known extractive methods.
The alternative approach of abstractive summarization...

There is a number of scientific works \cite{nenkova2011automatic, lloret2012text, saggion2013automatic} that provide an extensive overview of different automatic summarization methods that were proposed ever since the publication of the first paper on the topic by \cite{luhn1958automatic}.
In our paper, we focus on the most prominent algorithms that were proposed in the last several years and are currently considered to be state-of-the-art, as well as describe the techniques for summarization evaluation which are currently standardly used for assesment of these algorithms.


\section{Extractive methods}
Extractive summarization methods produce summaries by concatenating several sentences (text units) from the text being summarized exactly as they occur.
The main task of such systems is to determine which sentences are important and should therefore be included in the summary.
For many years extractive methods have been the main focus of researchers in the text summarization community.

In \cite{cheng2016neural} researchers presented a data-driven summarization framework based on neural networks and continuous sentence features.
They develop data-driven single-document summarization framework based on an hierarchical document encoder and an attention-based extractor.
Such architecture enables development of different classes of summarization models which can extract sentences or words.
The models can be trained on large scale datasets and learn informativeness features based on continuous representations without any access to linguistic annotation.
The authors tested their approach on two datasets and demonstrated results that are
comparable to the state of the art.

In \cite{yousefi2017text} researchers presented methods of extractive query-oriented single-document summarization using a deep auto-encoder (AE) to compute a feature space from the term-frequency (tf) input.
The presented approach is completely unsupervised and does not require queries for any stage of training.
Smaller local vocabularies, comparing to other methods, allow to reduce the training and deployment computational costs and thus make the approach more suitable for implementation on mobile devices.
The authors perform experiments on two email corpora designed for text summarization and observe results that are much better than those of other unsupervised extractive email summarisation techniques and are comparable to the best supervised approaches.

Otherwise, in the past years there has not been any substantial advancements in extractive text summarization, most publications only proposed some small improvements to the already long-existing extractive methods.
Researchers \cite{mehta2016extractive} assume that extractive summarization methods may have achieved their peak and propose two possible research advancement options:  1) making ensembles of extractive methods and 2) focusing on abstractive techniques.

\section{Abstractive methods}
The goal of automatic text summarization systems is to produce summaries as good as the ones created by humans.
However, when people produce summaries, in order to make them optimal in terms of content and linguistic quality, they tend to edit the text rather than just copy sentences from it as they are.
This is why, in order to emulate this process, generation of abstractive summaries is necessary.
Abstractive summarization methods produce summaries by rewriting the content in an input, as opposed to extractive methods that simply extract and concatenate important text units from it.
Therefore, abstractive summarization requires a deeper semantic and discourse interpretation of the text, as well as a novel text generation process.

Very recently, there have been several breakthroughs in abstractive text summarization using deep learning.
Most of the research relies on Sequence to Sequence Model \cite{sutskever2014sequence}, which was first introduced as Encoder-Decoder Model by \cite{cho2014learning} and later extended by \cite{bahdanau2014neural} with so-called "attention mechanism".
Sequence to Sequence (or seq2seq) model manages text generation pretty well as it was originally developed for machine translation.

A group of researchers at Facebook \cite{rush2015neural} presented a fully data-driven approach to abstractive sentence summarization.
Their method utilizes neural attention-based model that generates each word of the summary conditioned on the input sentence.
They combine this probabilistic model with a generation algorithm which produces accurate abstractive summaries.
The attention-based model provides less linguistic structure comparing to other abstractive summarization approaches, but is easily scalable for training on large amounts of data.
Furtermore, the lack of vocabulary constraints in the system makes it possible to train the model on diverse input-output pairs.
In addition to the paper, the source code \cite{Namas2015Facebook} was also provided to the public.

In later work \cite{chopra2016abstractive} researchers at Facebook extended their model to a recurrent neural network architecture.
The new model includes a more sophisticated encoder which explicitly encodes the position of the input words and uses convolutional network to encode input words.
With these modifications, the model showed to significantly outperform the previously proposed system on the Gigaword corpus and perform competitively on the DUC-2004 task.
Authors explain the distinctive improvement by the difference between tokenization of DUC-2004 and their training corpus, as well as by the fact that headlines in Gigaword are much shorter than in DUC-2004.

Researchers at IBM \cite{nallapati2016abstractive} also modeled abstractive text summarization using attentional encoder-decoder recurrent neural networks.
To address specific problems in abstractive summarization that are not sufficiently covered by the machine translation based model, they proposed several novel models, yielding further improvement in performance.
One of the most interesting models is called "Switching Generator/Pointer". In this model, the decoder is equipped with a "switch" that decides between generating a new word based on the context or using a word from the input.
In addition, in their work authors proposed a new dataset for multi sentence document summarization and established benchmark numbers on it.

Google has also proposed a sequence-to-sequence with attention model for text summarization, which they called "textsum".
The researchers did not provide a paper in support of their work, but openly published the source code \cite{LiuGoogle2016} at a hosting service.

Both approaches are currently the state-of-the-art on the DUC/TAC competition.

\cite{see2017get}

\section{Query-focused summarization}
Query-focused summarization became a task in DUC in 2004 in response to researchers' claims that generic summarization is too unconstrained and does not consider special user needs.
Such summarization allows to use more sophisticated, targeted approaches that integrate methods that seek specific types of information with data-driven, generic methods \cite{nenkova2011automatic}.

\cite{wang2016sentence} investigate the role of sentence compression techniques for query-focused multi-document summarization (MDS) and present a respective framework consisting of three steps: Sentence Ranking, Sentence Compression and Post-processing.
For sentence ranking, authors experiment with two ranking algorithms - Support Vector Regression (SVR) \cite{mozer1997} and LambdaMART \cite{burges2007learning}.
For sentence compression, which is the main focus of their work, they examine three different approaches to sentence-compression of their own design: rule-based, sequence-based and tree-based.
Finally, in the post-processing step, coreference resolution and sentence ordering are performed.
All of the proposed models show substantial improvement over pure extraction-based approaches for query-based MDS, with the best-performing system yielding an 11.02 ROUGE-2 score on the DUC 2006 dataset.

In \cite{litvak2017query} authors continued their earlier work \cite{litvak2015krimping} of applying the Minimum Description Length (MDL) principle for generic summarization by constructing a model where frequent word sets depend on the query.
The idea behind the MDL principle is that regularities in data can be used for its compression and the best hypothesis to describe these regularities is the one that can compress the data the most.
In the proposed approach, authors select frequent word sets that lead to the best compression of the data and therefore describe the document the best.
The extracted summary consist of sentences that provide the best coverage of query-related frequent word sets.
The summarization approach was evaluated using ROUGE-1 based on the DUC 2005 and DUC 2006 corpora that were specifically designed for query-based summarization and has shown to perform competitively with the best results. 

In their work, \cite{nema2017diversity} present a model for query-based summarization based on the encode-attend-decode paradigm with two additions: (1) a query attention model which learns to concentrate on different parts of the query at different points in time, and (2) a new diversity based attention model which aims mitigate the drawback of generating repeated phrases in the summary.

\section{APIs}

\section{Evaluation methods}
In this section we provide an overview of the main evaluation methods that are currently used to assess and report results on automatic summarization.

\subsection{Manual evaluation}
There are two manual methods used at TAC (Text Analysis Conference): Pyramid and Responsiveness.
Both these methods are focused at evaluating the informativeness and relevance of the summary content and do not assess its linguistic quality.

The Pyramid method \cite{nenkova2007pyramid} is based on the semantic analysis of multiple human models, which, taken together, according to the authors' assumption, yield a gold-standard for system output.
The method weights each Summary Content Unit (SCU) based on the number of human summaries in which it occurred.
The Pyramid score, which ranges from 0 to 1 and represents the informativeness of the summary, is equal to the ratio between the total SCUs weights in the created summary and the weight of an optimal summary with the same number of SCUs.

The Responsiveness metric is defined for query focused summarization.
For this evaluation, human reviewers are given a query and a summary and are asked to rate on a scale from 1 to 5 how good the summary is in terms of providing the requested information.

\subsection{Automatic evaluation}
Manual evaluations require a lot of time and effort, and are very expensive and difficult to conduct on a regular basis.

The most widely used automatic summarization evaluation measure for text summarization which is now standardly used to report results in research papers is called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) \cite{lin2004rouge}.
ROUGE was inspired by BLEU \cite{papineni2002bleu}, evaluation measure that was developed for machine translation evaluation.
The measure is based on the overlap of units such as n-gram, word sequences, and word pairs in the automatic summary and the optimal manual summaries created by annotators.
ROUGE is cheap and fast, and, unlike BLEU, that is oriented at precision, which is overly strict, ROUGE is recall-based, which makes it more preferable for summarization evaluation.

There are several variations of ROUGE. Below we shortly describe the ones that are most commonly used.
\begin{itemize}
\item ROUGE-n
\item ROUGE-L
\item ROUGE-SU
\end{itemize}

One of the main problems of ROUGE is that it relies purely on lexical overlaps, which can significantly underrate summarization score, especially in documents that have a lot of synonyms, terminology variations and paraphrasing.
Motivated by this observation, \cite{cohan2016revisiting} conducted a detailed  analysis of ROUGEâ€™s effectiveness for evaluation of scientific summaries.
In their results, ROUGE showed to be unreliable for evaluation of these types of summaries, with different ROUGE variations producing different correlations with the pyramid scores.
The authors propose a new metric for summarization evaluation called SERA (Summarization Evaluation by Relevance Analysis), which is based on the content relevance between automatically generated summary and the corresponding manual summaries, written by human annotators.
The newly proposed metric proved to be effective in evaluating summaries of scientific articles, consistently achieving hight correlations with manual scores.


% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2018}
\bibliography{acl2018}
\bibliographystyle{acl_natbib}

\appendix



\end{document}
