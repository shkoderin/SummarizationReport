%
% File acl2018.tex
%
%% Based on the style files for ACL-2017, with some changes, which were, in turn,
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper,onecolumn]{article}
\usepackage[hyperref]{acl2018}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Automatic Summarization: State-of-the-art review}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@tum.de} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@tum.de} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
This paper provides an overview of the most prominent algorithms for automatic summarization that were proposed in the last years.
\end{abstract}


\section{Introduction}
With large amounts of text available online, the amount of which is growing rapidly everyday, summarizing this content becomes necessary to help users handle such information overload and better perceive information.
Automatic summarization is the process of automatically shortening the content of an information source in a way that retains its most important information.
The goal of summarizers is to produce concise and fluent summary which would allow people to understand the content of the input without reading the entire text.
A good summary must be concise and fluent, capture all the important topics, but not contain repetitions of the same information.

For a long time extractive techniques of text summarization have been the primary focus of research in the field.
However, in the past years there have been less major advances in extractive text summarization.
Most of the research in this area proposes either an enhancement of one of the available extractive approaches, or an ensemble of several previously known extractive methods.
The alternative approach of abstractive summarization...

There is a number of scientific works \cite{nenkova2011automatic, lloret2012text, saggion2013automatic} that provide an extensive overview of different automatic summarization methods that were proposed ever since the publication of the first paper on the topic by \cite{luhn1958automatic}.
In our paper, we focus on the most prominent algorithms that were proposed in the last several years and are currently considered to be state-of-the-art, as well as describe the techniques for summarization evaluation which are currently standardly used for assesment of these algorithms.


\section{Extractive methods}
Extractive summarization methods produce summaries by concatenating several sentences (text units) from the text being summarized exactly as they occur.
The main task of such systems is to determine which sentences are important and should therefore be included in the summary.
For many years extractive methods have been the main focus of researchers in the text summarization community.

Many of the recent approaches address extractive summarization as a sequence labeling task, where each label indicates whether a sentence should be included in the summary.

In \cite{cheng2016neural} researchers presented a data-driven summarization framework based on neural networks and continuous sentence features.
They develop data-driven single-document summarization framework based on an hierarchical document encoder and an attention-based extractor.
Such architecture enables development of different classes of summarization models which can extract sentences or words.
The models can be trained on large scale datasets and learn informativeness features based on continuous representations without any access to linguistic annotation.
The labels are assigned to each sentence in the document individually based on their semantic correspondence with the gold summary.
The authors tested their approach on DailyMail and DUC 2002 datasets and demonstrated results that are comparable to the state of the art.

\cite{nallapati2017summarunner} present a RNN-based sequence model for extractive single-document summarization  which they call SummaRuNNer.
The approach is based on the idea of identifying the set of sentences which collectively give the highest ROUGE with respect to the gold summary.
The model allows visualization of its predictions broken up by abstract features such as information content, salience and novelty, thus being very interpretable.
Authors also present a novel training mechanism that allows the extractive model to be trained end-to-end using abstractive summaries.
The approach was evaluated on three datasets: Daily Mail, joint CNN/DailyMail and Out-of-Domain DUC 2002 corpus.
Results on DailyMail corpus were compared to the ones of \cite{cheng2016neural} using Rouge recall with summary length restricted to 75 and 275 bytes.
The extractively trained SummaRuNNer model showed significant improvement over the comparable model for summary length of 75 bytes, while performing comparably for summary length of 275 bytes.
The abstractively trained SummaRuNNer model performed comparably for summary length of 75 bytes, while underperforming the comparable model for summary length of 275 bytes.
On the joint CNN/Daily Mail corpus, SummaRuNNer significantly outperformed the abstractive model of \cite{nallapati2016abstractive}, which was the only work at the time, that reported performance on this dataset.
On the Out-of-Domain DUC 2002 corpus, SummaRuNNer was also on par with \cite{cheng2016neural}, however underperformed graph-based TGRAPH \cite{parveen2015topical} and URANK \cite{wan2010towards} algorithms that were state-of-the-art on this corpus at the time.

In \cite{narayan2017neural} authors use an architecture similar to those in previous approaches, however, for sentence ranking, they introduce a global optimization framework, which combines the maximum-likelihood cross-entropy loss with rewards from policy gradient reinforcement learning to directly optimize the final evaluation metric, ROUGE.
A sentence gets a high rank for summary selection if it often occurs in high scoring summaries.
The model was applied to the CNN/Daily Mail dataset, originally collected by \cite{hermann2015teaching}, and automatically evaluated using ROUGE, outperforming both of the above extractive systems, which are considered to be state-of-the art, as well as the most prominent abstractive systems \cite{nallapati2016abstractive, chen2016distraction, see2017get}, discussed in the next section.
Human evaluations showed that summaries, produced using this approach are also more informative and complete.

\cite{yasunaga2017graph} propose a graph-based neural multi-document summarization (MDS) based on the application of Graph Convolutional Networks (GCN) \cite{kipf2016semi} on sentence relation graphs.
GCN takes in sentence embeddings from Recurrent Neural Networks (RNN) as input node features and through multiple layer-wise propagation generates high-level hidden features for the sentences.
Sentence salience is then estimated through a regression on top and the important sentences are extracted in a greedy manner while avoiding redundancy.
The model was evaluated on the DUC 2004 multi-document summarization (MDS) task, outperforming traditional graph-based extractive summarizers and the vanilla GRU sequence model with no graph, as well performing competitively to other state-of-the-art MDS approaches.

In \cite{yousefi2017text} researchers presented methods of extractive query-oriented single-document summarization using a deep auto-encoder (AE) to compute a feature space from the term-frequency (tf) input.
The presented approach is completely unsupervised and does not require queries for any stage of training.
Smaller local vocabularies, comparing to other methods, allow to reduce the training and deployment computational costs and thus make the approach more suitable for implementation on mobile devices.
The authors perform experiments on two email corpora designed for text summarization and observe results that are much better than those of other unsupervised extractive email summarization techniques and are comparable to the best supervised approaches.

Otherwise, in the past years there has not been any substantial advancements in extractive text summarization, most publications only proposed some small improvements to the already long-existing extractive methods.
Researchers \cite{mehta2016extractive} assume that extractive summarization methods may have achieved their peak and propose two possible research advancement options:  1) making ensembles of extractive methods and 2) focusing on abstractive techniques.

\section{Abstractive methods} \label{abstractive}
The goal of automatic text summarization systems is to produce summaries as good as the ones created by humans.
However, when people produce summaries, in order to make them optimal in terms of content and linguistic quality, they tend to edit the text rather than just copy sentences from it as they are.
This is why, in order to emulate this process, generation of abstractive summaries is necessary.
Abstractive summarization methods produce summaries by rewriting the content in an input, as opposed to extractive methods that simply extract and concatenate important text units from it.
Therefore, abstractive summarization requires a deeper semantic and discourse interpretation of the text, as well as a novel text generation process.

Very recently, there have been several breakthroughs in abstractive text summarization using deep learning.
Most of the research relies on Sequence to Sequence Model \cite{sutskever2014sequence}, which was first introduced as Encoder-Decoder Model by \cite{cho2014learning} and later extended by \cite{bahdanau2014neural} with so-called "attention mechanism".
Encoder encodes source sequence into a context vector, while decoder decodes context vector to produce a target sequence. Attention mechanism is used to locate the focus region  during decoding.
Sequence to Sequence (or seq2seq) model manages text generation pretty well as it was originally developed for machine translation.

A group of researchers at Facebook \cite{rush2015neural} presented a fully data-driven approach to abstractive sentence summarization.
Their method utilizes neural attention-based model that generates each word of the summary conditioned on the input sentence.
They combine this probabilistic model with a generation algorithm which produces accurate abstractive summaries.
The attention-based model provides less linguistic structure comparing to other abstractive summarization approaches, but is easily scalable for training on large amounts of data.
Furtermore, the lack of vocabulary constraints in the system makes it possible to train the model on diverse input-output pairs.
In addition to the paper, the source code \cite{Namas2015Facebook} was also provided to the public.

In later work \cite{chopra2016abstractive} researchers at Facebook extended their model to a Recurrent Neural Network (RNN) architecture.
The new model includes a more sophisticated encoder which explicitly encodes the position of the input words and uses convolutional network to encode input words.
With these modifications, the model showed to significantly outperform the previously proposed system on the Gigaword corpus and perform competitively on the DUC-2004 task.
Authors explain the distinctive improvement by the difference between tokenization of DUC-2004 and their training corpus, as well as by the fact that headlines in Gigaword are much shorter than in DUC-2004.

Google has also proposed a sequence-to-sequence with attention model for text summarization, which they called "textsum".
The researchers did not provide a paper in support of their work, but openly published the source code \cite{LiuGoogle2016} at a hosting service.

Researchers at IBM \cite{nallapati2016abstractive} modeled abstractive text summarization using attentional encoder-decoder recurrent neural networks.
To address specific problems in abstractive summarization that are not sufficiently covered by the machine translation based model, they proposed several novel models, yielding further improvement in performance.
One of the most interesting models is called "Switching Generator/Pointer". In this model, the decoder is equipped with a "switch" that decides between generating a new word based on the context or using a word from the input.
In addition, in their work authors proposed a new dataset for multi sentence document summarization and established benchmark numbers on it.
The proposed summarization approach showed to significantly outperform the ones of \cite{rush2015neural} and \cite{chopra2016abstractive} and exhibit better abstractive ability.
Authors believe that their superior performance in comparison to the above methods was reached by using bidirectional RNN instead of the bag-of-embeddings representation to model the source, as this approach captures richer contextual information of every word.

Although these approaches are considered to be the state-of-the-art, they are far from perfect in that they sometimes inaccurately reproduce factual details, are unable to deal with out-of-vocabulary (OOV) words and can only deal with very short documents.
Researchers at Facebook use only the first sentence of the source document to train the model, while researchers at Google and IBM use two sentences from the source with a limit of 120 words.
However, summarization systems need to be able to deal with much longer documents.
In the works described next, authors tried to address these issues by proposing various modifications to the standard sequence-to-sequence model.

In their work, \cite{chen2016distraction} explore neural summarization technologies for new articles which contain thousands of words.
Researchers also base their model on the encoder-decoder framework, however, instead of focusing on attention to get the local context like most of the recent work does, they incorporate coverage mechanism, "distracting" the models to different parts of a document to avoid focusing on only one thing and get the full picture.
The authors do not restrict the encoders' architectures to the standard RNN, but use bi-directional gated recurrent units (bi-GRUs) architecture for encoding and decoding.
Without engineering any features, they train the models on two large datasets and test them on LCSTS corpus.
The proposed approach achieved better performance than the best result reported in \cite{hu2015lcsts}, which the authors used as a baseline.

\cite{see2017get} propose a novel architecture that enhances the standard sequence-to-sequence attentional model by using a hybrid pointer-to-pointer generator network and the coverage mechanism.
The proposed hybrid network is similar to the ones proposed for short-text summarization by \cite{gu2016incorporating} and \cite{miao2016language}.
It uses pointing \cite{vinyals2015pointer} to copy words from the input text, which provides better accuracy and is better in dealing with OOV words, while retaining the ability to generate new words.
To ensure coverage of the input document and thus reduce repetitions in the summary, authors propose a new version of the coverage vector by adapting the model of \cite{tu2016modeling}.
The model was applied to the CNN/Daily Mail dataset, outperforming the results of \cite{nallapati2016abstractive} by several ROUGE points.

\section{Query-focused summarization}
Query-focused summarization became a task in DUC in 2004 in response to researchers' claims that generic summarization is too unconstrained and does not consider special user needs.
The aim of such summarization is to generate a summary of a document or multiple documents in the context of a query.
Such summarization allows to use more sophisticated, targeted approaches that integrate methods that seek specific types of information with data-driven, generic methods \cite{nenkova2011automatic}.

\cite{wang2016sentence} investigate the role of sentence compression techniques for query-focused multi-document summarization (MDS) and present a respective framework consisting of three steps: Sentence Ranking, Sentence Compression and Post-processing.
For sentence ranking, authors experiment with two ranking algorithms - Support Vector Regression (SVR) \cite{mozer1997} and LambdaMART \cite{burges2007learning}.
For sentence compression, which is the main focus of their work, they examine three different approaches to sentence-compression of their own design: rule-based, sequence-based and tree-based.
Finally, in the post-processing step, coreference resolution and sentence ordering are performed.
All of the proposed models show substantial improvement over pure extraction-based approaches for query-based MDS, with the best-performing system yielding an 11.02 ROUGE-2 score on the DUC 2006 dataset.

In \cite{litvak2017query} authors continued their earlier work \cite{litvak2015krimping} of applying the Minimum Description Length (MDL) principle for generic summarization by constructing a model where frequent word sets depend on the query.
The idea behind the MDL principle is that regularities in data can be used for its compression and the best hypothesis to describe these regularities is the one that can compress the data the most.
In the proposed approach, authors select frequent word sets that lead to the best compression of the data and therefore describe the document the best.
The extracted summary consist of sentences that provide the best coverage of query-related frequent word sets.
The summarization approach was evaluated using ROUGE-1 based on the DUC 2005 and DUC 2006 corpora that were specifically designed for query-based summarization and has shown to perform competitively with the best results.

The highly successful Encoder-Decoder Model that was discussed in Section \ref{abstractive} was recently adapted to the query-based summarization by \cite{nema2017diversity}.
The authors present a model for abstractive query-based summarization based on the encode-attend-decode paradigm with two additions: (1) a query attention model which learns to concentrate on different parts of the query at different points in time, and (2) a new diversity based attention model which aims to mitigate the drawback of generating repeated phrases in the summary.
The authors also introduced a new dataset based on debatepedia and empirically proved that their approach performs significantly better than the plain (vanilla) encode-attend-decode mechanism, gaining 28\% in ROUGE-L score, and thus outperforming the previously proposed models.

\section{APIs}
Service Kong Inc. (until October 2017 - Mashape, Inc.), which offers open-source platforms and cloud services to manage, monitor and scale APIs, as well as a marketplace for discovering and publishing them, provides a variety of summarization APIs and rates the following as the best:
\begin{itemize}
\item DuckDuckGo Zero-Click Info API (2011) provides results for topic summaries, categories, disambiguation, and definitions. It gives direct links to other services (via !bang syntax), lists related topics and gives official sites when available.
\item ML Analyzer API (2013) includes a variety of endpoints, such as Article Summarizer, Language Detector, Adult Content Analyzer, Content Classifier, Names Extractor, Sentiment Analyzer, Location Extractor and Stocks Symbols Extractor.
\item Stremor RSS Summary API and Stremor TLDR for HTML or Text (2013) are are products of Stremor Corp., which develops and builds APIs for language processing. Their summarization APIs generate short summaries of long input, using Stremor's language heuristics engine, Liquid Helium.
\end{itemize}
However, all of these APIs are outdated and therefore cannot compete with the state-of-the-art algorithms.

Marcus L. Endicott conducted a  thorough research on the automatic summarization  implementations, publicly available on  the hosting service GitHub and compiled a list of the best 100 summarizers \cite{endicot2017Top100}.

Deep AI offers Machine Learning API \cite{deepaiapi} which, among others, includes a TextRank implementation for text summarization and keyword extraction by Summa NLP.
It provides 1.000 API requests for free every month.

AYLIEN Text Analysis API \cite{aylienapi} provides Natural Language Processing, Information Retrieval and Machine Learning tools for analyzing documents, news articles, Tweets and URLs in 6 languages: English, German, French, Italian, Spanish and Portuguese.
All of the Text Analysis API endpoints are available in the free plan, which allows to make 1.000 calls a day free of charge.

MeaningCloud provides a variety of text processing APIs, including Text Classification, Sentiment Analysis, Text Clustering and Summarization APIs for free \cite{meaaningcloud2017api}.
It is based on the extractive summarization approach, producing multilingual summaries of a desired length.

Powerful text analysis APIs are provided by a startup Indico \cite{indicoapi}.
It provides native integration with some major programming languages, including Python, Java, Ruby, R, Javascript and others.
Additionally, it is also possible to use the Indico suite of predictive APIs directly through RESTful API calls.
Text for summarization can be sent to the Indico text analysis functions as raw text strings or URLs.
Each Indico function can be called with a batch of data for analyzing many examples with a single network request, which significantly improves the speed of using the APIs.
Indico also includes a variety of models which can be used to analyze text in different contexts such as sentiment, references to people, places or organizations, relevance of query terms and phrases, language of the text, emotions expressed by the author etc.
In addition, Indico can be used for image analysis.
To use Indico's public API, a free account and an API key is required.
Indico provides three plans: Pay as You Go, Enterprise and Non-commercial.
Pay as You Go grants access to all standard text and image APIs with 10.000 free calls per month and every consequent call being chargeable.
The Enterprise plan provides access to the highest quality text and image APIs, with fixed price per month starting at \$5000 for millions of calls.
This plan also includes additional privileges such as private cloud, free consultations and access to the development team.
The Non-commercial plan, which is aimed at students and researchers, provides 10.000 free calls each month with the possibility to get extra calls at request.

Google Cloud Natural Language REST API \cite{googleapi} offers machine learning models which can be used for sentiment and syntax analysis, recognition of different entities including person, organization, location, events, products and media, as well as classification of content in more than 700 predefined categories.
Text can be uploaded in the request or integrated with Google Cloud Storage.
The features are multilingual, allowing to analyze text in multiple languages including English, Spanish, Japanese, Chinese (Simplified and Traditional), French, German, Italian, Korean and Portuguese.
The API does not provide automatic summarization functionality, however can obtain information, valuable for this task, such as sailence field in the analyzeEntities method or dependencyEdge fields in the analyzeSyntax method.
The price for consuming the API depends on the features used and is calculated per 1.000 units by monthly usage.

\section{Evaluation methods}
In this section we provide an overview of the main evaluation methods that are currently used to assess and report results on automatic summarization.

\subsection{Manual evaluation}
There are two manual methods used at TAC (Text Analysis Conference): Pyramid and Responsiveness.
Both these methods are focused at evaluating the informativeness and relevance of the summary content and do not assess its linguistic quality.

The Pyramid method \cite{nenkova2007pyramid} is based on the semantic analysis of multiple human models, which, taken together, according to the authors' assumption, yield a gold-standard for system output.
The method weights each Summary Content Unit (SCU) based on the number of human summaries in which it occurred.
The Pyramid score, which ranges from 0 to 1 and represents the informativeness of the summary, is equal to the ratio between the total SCUs weights in the created summary and the weight of an optimal summary with the same number of SCUs.

The Responsiveness metric is defined for query focused summarization.
For this evaluation, human reviewers are given a query and a summary and are asked to rate on a scale from 1 to 5 how good the summary is in terms of providing the requested information.

\subsection{Automatic evaluation}
Manual evaluations require a lot of time and effort, and are very expensive and difficult to conduct on a regular basis.

The most widely used automatic summarization evaluation measure for text summarization which is now standardly used to report results in research papers is called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) \cite{lin2004rouge}.
ROUGE was inspired by BLEU \cite{papineni2002bleu}, evaluation measure that was developed for machine translation evaluation.
The measure is based on the overlap of units such as n-gram, word sequences, and word pairs in the automatic summary and the optimal manual summaries created by annotators.
ROUGE is cheap and fast, and, unlike BLEU, that is oriented at precision, which is overly strict, ROUGE is recall-based, which makes it more preferable for summarization evaluation.

There are several variations of ROUGE. Below we shortly describe the ones that are most commonly used.
\begin{itemize}
\item ROUGE-N is based on an n-gram recall between a candidate summary and a set of reference summaries, i.e. the score is computed as the ratio between the number of common n-grams and the number of n-grams only in the reference summary.
\item ROUGE-L takes into account into account longest common subsequences to avoid the shortcoming of ROUGE-N when the measure may be based on sequences of text which are too small.
Advantages of this metric are that it only requires in-sequence matches but not consecutive matches, automatically includes longest in-sequence common n-grams and takes into account sentence level structure in a natural way.
\item ROUGE-SU is an extension of ROUGE-S which takes into account both bigrams and unigrams, thus avoiding assignment of 0 score to a sentence which does not share a skip bigram but instead has a common unigram.
Correlation analysis results on DUC summarization data show that ROUGE-SU correlates the best with human judgements.
\end{itemize}
All variations of ROUGE can be downloaded from a GutHub repository \cite{rougerepos}.

One of the main problems of ROUGE is that it relies purely on lexical overlaps, which can significantly underrate summarization score, especially in documents that have a lot of synonyms, terminology variations and paraphrasing.
Motivated by this observation, \cite{cohan2016revisiting} conducted a detailed  analysis of ROUGE’s effectiveness for evaluation of scientific summaries.
In their results, ROUGE showed to be unreliable for evaluation of these types of summaries, with different ROUGE variations producing different correlations with the pyramid scores.
The authors propose a new metric for summarization evaluation called SERA (Summarization Evaluation by Relevance Analysis), which is based on the content relevance between automatically generated summary and the corresponding manual summaries, written by human annotators.
The newly proposed metric proved to be effective in evaluating summaries of scientific articles, consistently achieving hight correlations with manual scores.


% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2018}
\bibliography{acl2018}
\bibliographystyle{acl_natbib}

\appendix



\end{document}
